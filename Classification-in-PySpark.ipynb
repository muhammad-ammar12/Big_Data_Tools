{"cells":[{"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aD11wt_m7Nh8","outputId":"50a98f58-ed7f-4012-9dbc-ea4a84be1ec4","executionInfo":{"status":"ok","timestamp":1684061402802,"user_tz":420,"elapsed":22309,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  libxtst6 openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n","  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n","  fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 36.5 MB of archives.\n","After this operation, 144 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libxtst6 amd64 2:1.2.3-1 [12.8 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 openjdk-8-jre-headless amd64 8u362-ga-0ubuntu1~20.04.1 [28.2 MB]\n","Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 openjdk-8-jdk-headless amd64 8u362-ga-0ubuntu1~20.04.1 [8,282 kB]\n","Fetched 36.5 MB in 2s (16.8 MB/s)\n","Selecting previously unselected package libxtst6:amd64.\n","(Reading database ... 122519 files and directories currently installed.)\n","Preparing to unpack .../libxtst6_2%3a1.2.3-1_amd64.deb ...\n","Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","Preparing to unpack .../openjdk-8-jre-headless_8u362-ga-0ubuntu1~20.04.1_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u362-ga-0ubuntu1~20.04.1_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n","Setting up libxtst6:amd64 (2:1.2.3-1) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u362-ga-0ubuntu1~20.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"]}]},{"cell_type":"markdown","source":["Now install Spark 3.2.1 with Hadoop 2.7"],"metadata":{"id":"7-gD52aV7R7s"}},{"cell_type":"code","source":["!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04gzkksJ7WBY","outputId":"fbab25e7-49a9-4edf-9e26-625a0563e3b7","executionInfo":{"status":"ok","timestamp":1684061426874,"user_tz":420,"elapsed":10904,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-05-14 10:50:09--  https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n","Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n","Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 272637746 (260M) [application/x-gzip]\n","Saving to: ‘spark-3.2.1-bin-hadoop2.7.tgz’\n","\n","spark-3.2.1-bin-had 100%[===================>] 260.01M  27.8MB/s    in 10s     \n","\n","2023-05-14 10:50:20 (25.9 MB/s) - ‘spark-3.2.1-bin-hadoop2.7.tgz’ saved [272637746/272637746]\n","\n"]}]},{"cell_type":"markdown","source":[" we just need to unzip that folder.\n"],"metadata":{"id":"EHRa3Ewe7ZXg"}},{"cell_type":"code","source":["!tar xf /content/spark-3.2.1-bin-hadoop2.7.tgz\n"],"metadata":{"id":"eI9gfk337a00"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is one last thing that we need to install and that is the findspark library. It will locate Spark on the system and import it as a regular library.\n"],"metadata":{"id":"sxJyruvY7s10"}},{"cell_type":"code","source":["!pip install -q findspark\n"],"metadata":{"id":"6SJCgVHL7tM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\""],"metadata":{"id":"YZzbCbhc8URd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We need to locate Spark in the system. For that, we import findspark and use the findspark.init() method."],"metadata":{"id":"GNmcH1W-8XRx"}},{"cell_type":"code","source":["import findspark\n","findspark.init()\n","findspark.find()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"DohPJLoD8Y75","outputId":"44d26483-f149-4207-f6ef-8fc7ec44448a","executionInfo":{"status":"ok","timestamp":1684061491743,"user_tz":420,"elapsed":261,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/spark-3.2.1-bin-hadoop2.7'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment."],"metadata":{"id":"fs3YxjYb8SIx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dHOL9iB59TM"},"outputs":[],"source":["import pyspark\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arH_LBjQ59TQ","outputId":"5fb0f6cd-9d1f-4d3b-ccd6-c3baa4f7251e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684062501411,"user_tz":420,"elapsed":1502,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["+------+-------------+---------+------------+----------------+----------+-----------------------------+------------------+-------+----+--------------+-----------------------------------------+---+\n","|userId|userSessionId|teamLevel|platformType|count_gameclicks|count_hits|Number_of_purchase(by a user)|avg_Purchase_price|country|team|Purchase_price|duration_btw_1stplayed_&_purchase(months)|age|\n","+------+-------------+---------+------------+----------------+----------+-----------------------------+------------------+-------+----+--------------+-----------------------------------------+---+\n","|   937|         5652|        1|     android|              39|         0|                            1|               1.0|     ZM|  11|             1|                                       25| 56|\n","|  1623|         5659|        1|      iphone|             129|         9|                            1|              10.0|     SV|  13|            10|                                       29| 75|\n","|    83|         5661|        1|     android|             102|        14|                            1|               5.0|     GF|  63|             5|                                       10| 35|\n","|   121|         5665|        1|     android|              39|         4|                            1|               3.0|     UY|  18|             3|                                       10| 58|\n","|   462|         5666|        1|     android|              90|        10|                            1|               3.0|     PH|  63|             3|                                       30| 63|\n","+------+-------------+---------+------------+----------------+----------+-----------------------------+------------------+-------+----+--------------+-----------------------------------------+---+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.master('local[*]').appName('flemingo').getOrCreate()\n","\n","# Read data from CSV file\n","#you can download it from here: https://raw.githubusercontent.com/besherh/BigDataManagement/main/SparkCSV/flights-larger.csv\n","data = spark.read.csv('/content/data.csv', sep=',', header=True, inferSchema=True, nullValue='NA')\n","\n","data.show(5)\n"]},{"cell_type":"markdown","source":["#Total number of records"],"metadata":{"id":"AgbyF6LQ-wC4"}},{"cell_type":"code","source":["data.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MqQaNfM--yk-","outputId":"8caf1f7d-4689-461a-b999-2664750358c9","executionInfo":{"status":"ok","timestamp":1684062511319,"user_tz":420,"elapsed":949,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2375"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNXqyikC59TR","outputId":"c21705d8-f3f3-426d-ec62-6b8f8817ea9c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684062684206,"user_tz":420,"elapsed":683,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":14}],"source":["# Remove the 'flight' column\n","data = data.drop('userid')\n","\n","# Number of records with missing 'delay' values\n","data.filter('platformtype IS NULL').count()\n","\n"]},{"cell_type":"code","source":["# Remove records with missing values in any column and get the number of remaining rows\n","data = data.dropna()\n","print(data.count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IAap-rhc-OyT","executionInfo":{"status":"ok","timestamp":1684063023620,"user_tz":420,"elapsed":1237,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}},"outputId":"3931dbc3-7323-42b0-d728-c2039adc30f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2363\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcKlOYT959TV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684063796104,"user_tz":420,"elapsed":1310,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}},"outputId":"60d2f163-1131-4700-b85e-8be8f0c178cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+----------------+----------+-----------------------------+------------------+----+--------------+-----------------------------------------+---+-----------+------+\n","|teamLevel|count_gameclicks|count_hits|Number_of_purchase(by a user)|avg_Purchase_price|team|Purchase_price|duration_btw_1stplayed_&_purchase(months)|age|country_idx|target|\n","+---------+----------------+----------+-----------------------------+------------------+----+--------------+-----------------------------------------+---+-----------+------+\n","|        1|              39|         0|                            1|               1.0|  11|             1|                                       25| 56|       26.0|   1.0|\n","|        1|             129|         9|                            1|              10.0|  13|            10|                                       29| 75|       14.0|   0.0|\n","|        1|             102|        14|                            1|               5.0|  63|             5|                                       10| 35|       16.0|   1.0|\n","|        1|              39|         4|                            1|               3.0|  18|             3|                                       10| 58|       83.0|   1.0|\n","|        1|              90|        10|                            1|               3.0|  63|             3|                                       30| 63|       46.0|   1.0|\n","+---------+----------------+----------+-----------------------------+------------------+----+--------------+-----------------------------------------+---+-----------+------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.ml.feature import StringIndexer\n","\n","# Create an indexer\n","indexer = StringIndexer(inputCol='country', outputCol='country_idx')\n","\n","# Indexer identifies categories in the data\n","indexer_model = indexer.fit(data)\n","\n","# Indexer creates a new column with numeric index values\n","indexed_data = indexer_model.transform(data)\n","\n","# Repeat the process for the other categorical feature\n","indexed_data = StringIndexer(inputCol='platformType', outputCol='target').fit(indexed_data).transform(indexed_data)\n","indexed_data =indexed_data.drop('userSessionId','platformType','country')\n","\n","indexed_data.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0AyGMFP59TW","outputId":"a248c315-59bd-4a12-9eba-730f15094576","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684065039651,"user_tz":420,"elapsed":1100,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------------------------------+------+\n","|features                                         |target|\n","+-------------------------------------------------+------+\n","|[1.0,39.0,0.0,1.0,1.0,11.0,1.0,25.0,56.0,26.0]   |1.0   |\n","|[1.0,129.0,9.0,1.0,10.0,13.0,10.0,29.0,75.0,14.0]|0.0   |\n","|[1.0,102.0,14.0,1.0,5.0,63.0,5.0,10.0,35.0,16.0] |1.0   |\n","|[1.0,39.0,4.0,1.0,3.0,18.0,3.0,10.0,58.0,83.0]   |1.0   |\n","|[1.0,90.0,10.0,1.0,3.0,63.0,3.0,30.0,63.0,46.0]  |1.0   |\n","+-------------------------------------------------+------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.ml.feature import VectorAssembler\n","\n","# Create an assembler object\n","assembler = VectorAssembler(inputCols=[\n","    'teamLevel', 'count_gameclicks', 'count_hits',\n","    'Number_of_purchase(by a user)', \n","    'avg_Purchase_price','team',\n","    'Purchase_price', 'duration_btw_1stplayed_&_purchase(months)', 'age','country_idx'\n","], outputCol='features')\n","\n","# Consolidate predictor columns\n","assembled_data = assembler.transform(indexed_data)\n","\n","# Check the resulting column\n","new_data=assembled_data.select('features', 'target').show(5, truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"MX9boxPM59TX"},"source":["## Decision Tree\n"]},{"cell_type":"markdown","metadata":{"id":"uUhBIZUo59TX"},"source":["### Train/test split\n","To objectively assess a Machine Learning model you need to be able to test it on an independent set of data. You can't use the same data that you used to train the model: of course the model will perform (relatively) well on those data!\n","\n","You will split the data into two components:\n","\n","- training data (used to train the model) and\n","- testing data (used to test the model)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"36a8TMVZ59TX","outputId":"af927285-1f29-44d2-fa4b-aa02be06d1e6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684065279299,"user_tz":420,"elapsed":1181,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.8087177316969953\n","+--------------------+------+\n","|            features|target|\n","+--------------------+------+\n","|[1.0,8.0,1.0,2.0,...|   2.0|\n","|[1.0,8.0,1.0,2.0,...|   2.0|\n","|[1.0,19.0,2.0,1.0...|   1.0|\n","|[1.0,22.0,1.0,1.0...|   0.0|\n","|[1.0,27.0,4.0,1.0...|   0.0|\n","+--------------------+------+\n","only showing top 5 rows\n","\n"]}],"source":["# Split into training and test sets in a 80:20 ratio\n","train, test = assembled_data.randomSplit([0.8, 0.2], seed=1)\n","\n","# Check that training set has around 80% of records\n","ratio = train.count() / assembled_data.count()\n","train_data=train.select('features', 'target')\n","test_data=test.select('features', 'target')\n","print(ratio)\n","train_data.show(5)"]},{"cell_type":"markdown","metadata":{"id":"V5vo8kFk59TX"},"source":["### Build a Decision Tree\n","Now that you've split the flights data into training and testing sets, you can use the training set to fit a Decision Tree model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zj4G37ds59TY","outputId":"da285ee4-7817-4f2a-c09d-1e329769c683","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684065933222,"user_tz":420,"elapsed":2282,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["+------+----------+---------------------------------------------------------------------------------------+\n","|target|prediction|probability                                                                            |\n","+------+----------+---------------------------------------------------------------------------------------+\n","|0.0   |0.0       |[0.9240710823909531,0.050080775444264945,0.011308562197092083,0.0,0.014539579967689823]|\n","|0.0   |0.0       |[0.9240710823909531,0.050080775444264945,0.011308562197092083,0.0,0.014539579967689823]|\n","|0.0   |0.0       |[0.9240710823909531,0.050080775444264945,0.011308562197092083,0.0,0.014539579967689823]|\n","|0.0   |0.0       |[0.7,0.3,0.0,0.0,0.0]                                                                  |\n","|0.0   |0.0       |[0.9240710823909531,0.050080775444264945,0.011308562197092083,0.0,0.014539579967689823]|\n","+------+----------+---------------------------------------------------------------------------------------+\n","only showing top 5 rows\n","\n"]}],"source":["from pyspark.ml.classification import DecisionTreeClassifier\n","\n","# Create a classifier object and fit to the training data\n","tree = DecisionTreeClassifier(labelCol='target')\n","tree.setMaxBins(216)\n","tree_model = tree.fit(train_data)\n","\n","# Create predictions for the testing data and take a look at the predictions\n","prediction = tree_model.transform(test_data)\n","prediction.select('target', 'prediction', 'probability').show(5, False)"]},{"cell_type":"markdown","metadata":{"id":"DAgFCt6859TY"},"source":["### Evaluate the Decision Tree\n","You can assess the quality of your model by evaluating how well it performs on the testing data. Because the model was not trained on these data, this represents an objective assessment of the model.\n","\n","A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of:\n","\n","- True Negatives (TN) — model predicts negative outcome & known outcome is negative\n","- True Positives (TP) — model predicts positive outcome & known outcome is positive\n","- False Negatives (FN) — model predicts negative outcome but known outcome is positive\n","- False Positives (FP) — model predicts positive outcome but known outcome is negative."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TysO-BA159TZ","outputId":"30cf7516-05e2-489c-d4ae-b2c5381a61e2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684066120491,"user_tz":420,"elapsed":2182,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["+------+----------+-----+\n","|target|prediction|count|\n","+------+----------+-----+\n","|   2.0|       0.0|    2|\n","|   1.0|       1.0|  142|\n","|   3.0|       2.0|   20|\n","|   0.0|       1.0|   20|\n","|   0.0|       4.0|    1|\n","|   2.0|       2.0|   47|\n","|   1.0|       0.0|    6|\n","|   3.0|       1.0|    3|\n","|   2.0|       1.0|   16|\n","|   1.0|       2.0|   11|\n","|   0.0|       0.0|  161|\n","|   1.0|       3.0|    1|\n","|   4.0|       0.0|    1|\n","|   3.0|       3.0|   11|\n","|   3.0|       0.0|    1|\n","|   4.0|       1.0|    9|\n","+------+----------+-----+\n","\n","0.9209726443768997\n"]}],"source":["# Create a confusion matrix\n","prediction.groupBy('target', 'prediction').count().show()\n","\n","# Calculate the elements of the confusion matrix\n","TN = prediction.filter('prediction = 0 AND target = 0').count()\n","TP = prediction.filter('prediction = 1 AND target = 1').count()\n","FN = prediction.filter('prediction = 0 AND target = 1').count()\n","FP = prediction.filter('prediction = 1 AND target = 0').count()\n","\n","# Accuracy measures the proportion of correct predictions\n","accuracy = (TN + TP) / (TN + TP + FN + FP)\n","print(accuracy)"]},{"cell_type":"code","source":["precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","print('precision = {:.2f}\\nrecall   = {:.2f}'.format(precision, recall))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FM7kc82UKta-","executionInfo":{"status":"ok","timestamp":1684066201012,"user_tz":420,"elapsed":315,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}},"outputId":"febc03d4-73cb-442d-ee30-aeafc9590c1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["precision = 0.88\n","recall   = 0.96\n"]}]},{"cell_type":"code","source":["from re import VERBOSE\n","from pyspark.ml.classification import MultilayerPerceptronClassifier\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","# Define the layers of the MLP\n","layers = [10, 10,15, 5]\n","\n","# Set the number of epochs and the dropout rate\n","num_epochs = 500\n","\n","\n","# Create the MLP object and set the parameters\n","mlp = MultilayerPerceptronClassifier(featuresCol='features', labelCol='target', layers=layers, seed=42,\n","                                     maxIter=num_epochs, blockSize=500, stepSize=0.1)\n","\n","# Train the MLP on the training data\n","mlp_model = mlp.fit(train_data)\n","\n","# Make predictions on the test data\n","predictions = mlp_model.transform(test_data)\n","# Evaluate the performance of the MLP using the accuracy metric\n","evaluator = MulticlassClassificationEvaluator(labelCol='target', metricName='accuracy')\n","accuracy = evaluator.evaluate(predictions)\n","print('Accuracy = {:.2f}%'.format(accuracy * 100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VP5aykORNB4G","executionInfo":{"status":"ok","timestamp":1684070476847,"user_tz":420,"elapsed":18293,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}},"outputId":"b048f73f-66cb-4405-cb50-9c31dec8addd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy = 77.88%\n"]}]},{"cell_type":"code","source":["!pip install sparkdl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FuvzoMskWwnZ","executionInfo":{"status":"ok","timestamp":1684069375771,"user_tz":420,"elapsed":6224,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}},"outputId":"4583f05e-feee-4648-b86f-08efa68894f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sparkdl\n","  Downloading sparkdl-0.2.2-py3-none-any.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sparkdl\n","Successfully installed sparkdl-0.2.2\n"]}]},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.classification import MultilayerPerceptronClassifier\n","from sparkdl import KerasImageFileTransformer\n","\n","# Define the Keras model\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","keras_model = Sequential()\n","keras_model.add(Dense(10, input_shape=(10,), activation='relu'))\n","keras_model.add(Dense(5, activation='softmax'))\n","\n","# Define the Spark pipeline\n","\n","transformer = KerasImageFileTransformer(inputCol=\"features\", outputCol=\"predictions\", modelFile=keras_model, \\\n","                                         imageShape=(10,), outputMode=\"vector\")\n","mlp = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol=\"target\", predictionCol=\"prediction\", \\\n","                                      maxIter=100, layers=[10, 50, 5], blockSize=128, seed=1234)\n","pipeline = Pipeline(stages=[assembled_data, transformer, mlp])\n","\n","# Fit the model on the training data\n","model = pipeline.fit(train_data)\n","\n","# Make predictions on the test data\n","predictions = model.transform(test_data)\n","\n","# Evaluate the model\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","print(\"Test Accuracy = {}\".format(accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":693},"id":"17KH135vVhT0","executionInfo":{"status":"error","timestamp":1684069392262,"user_tz":420,"elapsed":13565,"user":{"displayName":"Muhammad Ammar Shahid","userId":"02279072261865309171"}},"outputId":"4b16b0ca-4c55-4b85-aec7-d067fc06f1fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function JavaWrapper.__del__ at 0x7f7fddd7f0a0>\n","Traceback (most recent call last):\n","  File \"/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 39, in __del__\n","    if SparkContext._active_spark_context and self._java_obj is not None:\n","AttributeError: 'MultilayerPerceptronClassifier' object has no attribute '_java_obj'\n","Exception ignored in: <function JavaWrapper.__del__ at 0x7f7fddd7f0a0>\n","Traceback (most recent call last):\n","  File \"/content/spark-3.2.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 39, in __del__\n","    if SparkContext._active_spark_context and self._java_obj is not None:\n","AttributeError: 'MultilayerPerceptronClassifier' object has no attribute '_java_obj'\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-ee1aced6297f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultilayerPerceptronClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasImageFileTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the Keras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sparkdl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimageIO\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasImageFileTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepImagePredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeepImageFeaturizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFImageTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sparkdl/transformers/keras_image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mkeyword_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHasInputCol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHasOutputCol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     CanLoadImage, HasKerasModel, HasOutputMode)\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFImageTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sparkdl/transformers/tf_image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorframes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorframes'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"erU7yX1KWsiF"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[{"file_id":"1QPG6nnmOX7QHEtBGGnc13aRpt3Sh-ygH","timestamp":1684061342512},{"file_id":"https://github.com/besherh/BigDataManagement/blob/main/SparkNotebooks/Classification_in_PySpark.ipynb","timestamp":1678994243436}]}},"nbformat":4,"nbformat_minor":0}